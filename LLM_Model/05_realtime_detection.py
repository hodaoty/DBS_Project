import pandas as pd
import os
import joblib
from datetime import datetime, timedelta
import numpy as np

# ----------------------------------------------------------------------
# A. Cáº¤U HÃŒNH ÄÆ¯á»œNG DáºªN VÃ€ THAM Sá»
# ----------------------------------------------------------------------
timestamp_str = datetime.now().strftime('%Y%m%d')
BASE_DIR = os.path.dirname(os.path.abspath(__file__))
# âš ï¸ ÄIá»€U CHá»ˆNH: Cáº§n tÃ¬m thÆ° má»¥c trained_model tá»« BASE_DIR (thÆ°á»ng lÃ  05_realtime_detection.py)
MODEL_DIR = os.path.join(BASE_DIR, 'trained_model') 

# TÃªn file model vÃ  scaler (Sá»­ dá»¥ng ngÃ y thÃ¡ng má»›i nháº¥t Ä‘á»ƒ tÃ¬m)
MODEL_FILE_NAME = f'isolation_forest_model-{timestamp_str}.pkl'
SCALER_FILE_NAME = f'scaler-{timestamp_str}.pkl'

MODEL_PATH = os.path.join(MODEL_DIR, MODEL_FILE_NAME)
SCALER_PATH = os.path.join(MODEL_DIR, SCALER_FILE_NAME)

# Táº§n suáº¥t gom nhÃ³m pháº£i khá»›p vá»›i bÆ°á»›c huáº¥n luyá»‡n (02_preprocessing.py)
RESAMPLE_FREQUENCY = '5T' 

# ----------------------------------------------------------------------
# B. HÃ€M Táº¢I MODEL VÃ€ SCALER
# ----------------------------------------------------------------------
def load_detection_assets():
    """Táº£i Isolation Forest Model vÃ  Standard Scaler Ä‘Ã£ huáº¥n luyá»‡n."""
    try:
        model = joblib.load(MODEL_PATH)
        scaler = joblib.load(SCALER_PATH)
        print(f"âœ… Táº£i thÃ nh cÃ´ng Model tá»«: {MODEL_PATH}")
        print(f"âœ… Táº£i thÃ nh cÃ´ng Scaler tá»«: {SCALER_PATH}")
        return model, scaler
    except FileNotFoundError:
        print("âŒ Lá»–I: KhÃ´ng tÃ¬m tháº¥y file mÃ´ hÃ¬nh hoáº·c scaler.")
        print("Vui lÃ²ng Ä‘áº£m báº£o Ä‘Ã£ cháº¡y 02_preprocessing.py vÃ  03_model_training.py thÃ nh cÃ´ng.")
        return None, None
    except Exception as e:
        print(f"âŒ Lá»–I khi táº£i assets: {e}")
        return None, None

# ----------------------------------------------------------------------
# C. HÃ€M MÃ” PHá»ŽNG Táº O Dá»® LIá»†U LOG Má»šI (Real-time/Next Batch)
# ----------------------------------------------------------------------
# Láº¥y Ä‘á»™ dÃ i cá»­a sá»• thá»i gian
window_duration = pd.Timedelta(RESAMPLE_FREQUENCY)

def generate_new_log_batch(reference_df, start_time, end_time, scenario="normal"):
    """
    Táº¡o má»™t batch dá»¯ liá»‡u log giáº£ Ä‘á»‹nh (chÆ°a Ä‘Æ°á»£c xá»­ lÃ½) cho má»™t cá»­a sá»• thá»i gian.
    """
    # Láº¥y cÃ¡c sá»± kiá»‡n log thá»±c táº¿ trong khung thá»i gian tham chiáº¿u
    sample_df = reference_df[
        (reference_df['timestamp'] >= start_time) & (reference_df['timestamp'] < end_time)
    ].copy()

    if sample_df.empty:
        # Táº¡o dá»¯ liá»‡u log tá»‘i thiá»ƒu náº¿u khÃ´ng cÃ³ máº«u thá»±c táº¿
        data = {
            'pid': [0], 'user': [None], 'database': [None], 'event_type': ['LOG'],
            'session_duration_sec': [0.0], 'query_command': [None], 
            'query_text': [None], 'timestamp': [start_time]
        }
        sample_df = pd.DataFrame(data)
        
    if scenario == "stress":
        # MÃ´ phá»ng báº¥t thÆ°á»ng: TÄƒng Ä‘á»™t biáº¿n lá»—i FATAL vÃ  káº¿t ná»‘i
        
        # 1. TÄƒng sá»‘ lÆ°á»£ng káº¿t ná»‘i/sá»± kiá»‡n
        sample_df = pd.concat([sample_df] * 5, ignore_index=True)
        
        # 2. ThÃªm lá»—i FATAL/ERROR
        error_events = []
        error_count = int(len(sample_df) * 0.1) # 10% sá»± kiá»‡n lÃ  lá»—i
        for i in range(error_count):
            error_time = start_time + timedelta(seconds=np.random.rand() * window_duration.total_seconds())
            error_events.append({
                'pid': 9999, 'user': 'attack', 'database': 'critical_db', 
                'event_type': 'FATAL', 'session_duration_sec': 0.0, 
                'query_command': 'SELECT', 'query_text': 'UNAUTHORIZED ACCESS', 
                'timestamp': error_time
            })
        sample_df = pd.concat([sample_df, pd.DataFrame(error_events)], ignore_index=True)
        
        print("âš ï¸ Ká»‹ch báº£n 'stress' (FATAL/Káº¿t ná»‘i) Ä‘Ã£ Ä‘Æ°á»£c mÃ´ phá»ng.")
        
    # Äáº·t láº¡i index theo timestamp vÃ  sáº¯p xáº¿p
    sample_df = sample_df.sort_values(by='timestamp').set_index('timestamp', drop=False)
    return sample_df

# ----------------------------------------------------------------------
# D. HÃ€M Táº O Äáº¶C TRÆ¯NG CHUá»–I THá»œI GIAN (Phá»ng theo 02_preprocessing.py)
# ----------------------------------------------------------------------
# âš ï¸ LÆ°u Ã½: HÃ m nÃ y pháº£i giá»‘ng há»‡t hÃ m trong 02_preprocessing.py Ä‘á»ƒ Ä‘áº£m báº£o tÃ­nh nháº¥t quÃ¡n
def create_time_series_features(df, freq=RESAMPLE_FREQUENCY):
    """
    Táº¡o cÃ¡c Ä‘áº·c trÆ°ng chuá»—i thá»i gian báº±ng cÃ¡ch gom nhÃ³m dá»¯ liá»‡u log.
    (Giá»‘ng há»‡t logic trong 02_preprocessing.py)
    """
    
    # Loáº¡i bá» cÃ¡c cá»™t ID khÃ´ng pháº£i Ä‘áº·c trÆ°ng khá»i quÃ¡ trÃ¬nh thá»‘ng kÃª
    # Äáº£m báº£o index lÃ  timestamp trÆ°á»›c khi drop
    df_temp = df.copy().set_index('timestamp', drop=True) 

    features_df = df_temp.drop(columns=['pid', 'user', 'database', 'query_command', 'query_text'], errors='ignore')
    
    # 1. Táº¡o Äáº·c trÆ°ng Äáº¿m (Count Features)
    count_features = features_df['event_type'].groupby(level=0).value_counts().unstack(fill_value=0)
    count_features.columns = [f'count_{col.lower().replace(" ", "_")}' for col in count_features.columns]
    
    # ThÃªm tá»•ng sá»‘ sá»± kiá»‡n (dÃ¹ng cho ratio)
    count_features['count_total_events'] = count_features.sum(axis=1)

    # 2. Táº¡o Äáº·c trÆ°ng Thá»i gian (Time Features)
    time_features = features_df['session_duration_sec'].resample(freq).agg({
        'avg_session_duration': 'mean',
        'max_session_duration': 'max',
        'total_session_time': 'sum'
    }).fillna(0)

    # 3. Gom nhÃ³m Äáº·c trÆ°ng
    final_features_df = count_features.resample(freq).sum().fillna(0)
    final_features_df = final_features_df.join(time_features)

    # 4. Táº¡o Äáº·c trÆ°ng Tá»· lá»‡ (Ratio Features)
    final_features_df['ratio_fatal_to_total'] = (
        final_features_df.get('count_fatal', 0) / final_features_df['count_total_events']
    ).fillna(0)
    
    # Loáº¡i bá» cá»™t 'count_total_events' vÃ  cÃ¡c cá»™t khÃ´ng cáº§n thiáº¿t khÃ¡c
    final_features_df = final_features_df.drop(columns=['count_total_events'], errors='ignore')

    return final_features_df

# ----------------------------------------------------------------------
# E. HÃ€M PHÃT HIá»†N Báº¤T THÆ¯á»œNG (CHÃNH)
# ----------------------------------------------------------------------
def detect_anomalies_realtime(model, scaler, new_log_data_df):
    """
    Ãp dá»¥ng pipeline tiá»n xá»­ lÃ½ vÃ  mÃ´ hÃ¬nh Ä‘á»ƒ phÃ¡t hiá»‡n báº¥t thÆ°á»ng trÃªn dá»¯ liá»‡u má»›i.
    """
    if new_log_data_df.empty:
        print("KhÃ´ng cÃ³ dá»¯ liá»‡u log má»›i Ä‘á»ƒ xá»­ lÃ½.")
        return None

    # 1. Ká»¹ thuáº­t Äáº·c trÆ°ng (Gom nhÃ³m 5T)
    features_df = create_time_series_features(new_log_data_df)
    
    # 2. Sáº¯p xáº¿p láº¡i cá»™t vÃ  Ä‘iá»n 0 cho cÃ¡c Ä‘áº·c trÆ°ng bá»‹ thiáº¿u
    if not hasattr(scaler, 'feature_names_in_') or scaler.feature_names_in_ is None:
        print("âŒ Lá»–I: Scaler khÃ´ng cÃ³ danh sÃ¡ch tÃªn Ä‘áº·c trÆ°ng. Vui lÃ²ng kiá»ƒm tra 02_preprocessing.py.")
        return None
        
    all_feature_names = list(scaler.feature_names_in_)
    
    # ThÃªm cÃ¡c cá»™t cÃ²n thiáº¿u vÃ o features_df vá»›i giÃ¡ trá»‹ 0
    for col in all_feature_names:
        if col not in features_df.columns:
            features_df[col] = 0.0
            
    # Sáº¯p xáº¿p láº¡i cá»™t theo thá»© tá»± Ä‘Ã£ huáº¥n luyá»‡n
    features_df = features_df[all_feature_names]

    # 3. Chuáº©n hÃ³a dá»¯ liá»‡u (Ãp dá»¥ng Scaler Ä‘Ã£ táº£i - KHÃ”NG dÃ¹ng fit_transform)
    scaled_data = scaler.transform(features_df)
    scaled_df = pd.DataFrame(scaled_data, index=features_df.index, columns=features_df.columns)

    # 4. Dá»± Ä‘oÃ¡n
    # âš ï¸ FIX: Sá»­ dá»¥ng báº£n sao sáº¡ch (18 cá»™t) cho cáº£ hai hÃ m dá»± Ä‘oÃ¡n
    X_predict_features = scaled_df.copy()

    scaled_df['anomaly_score'] = model.decision_function(X_predict_features)
    scaled_df['anomaly'] = model.predict(X_predict_features) # <--- ÄÃ£ sá»­a lá»—i: DÃ¹ng X_predict_features

    # Lá»c ra cÃ¡c báº£n ghi báº¥t thÆ°á»ng
    anomalies = scaled_df[scaled_df['anomaly'] == -1].copy()

    return anomalies

# ----------------------------------------------------------------------
# F. LOGIC THá»°C THI CHÃNH
# ----------------------------------------------------------------------
if __name__ == "__main__":
    
    # 1. Táº£i Model vÃ  Scaler
    anomaly_model, scaler = load_detection_assets()
    
    if anomaly_model is None or scaler is None:
        exit()

    # 2. Táº£i dá»¯ liá»‡u sá»± kiá»‡n chi tiáº¿t gá»‘c Ä‘á»ƒ lÃ m máº«u
    CSV_DIR_EVENT = os.path.join(BASE_DIR, '..', 'CSV_FILE', 'OUTPUT_CSVFILE', 'LOG_EVENT')
    EVENTS_CSV_PATH = os.path.join(CSV_DIR_EVENT, f'postgresql_events-{timestamp_str}.csv') 
    
    if not os.path.exists(EVENTS_CSV_PATH):
        print(f"âŒ Cáº¢NH BÃO: KhÃ´ng tÃ¬m tháº¥y file sá»± kiá»‡n gá»‘c táº¡i {EVENTS_CSV_PATH}. KhÃ´ng thá»ƒ mÃ´ phá»ng.")
        exit()
    
    full_events_df = pd.read_csv(EVENTS_CSV_PATH, parse_dates=['timestamp'])
    # Äáº·t index lÃ  timestamp vÃ  giá»¯ láº¡i cá»™t timestamp
    full_events_df = full_events_df.set_index('timestamp', drop=False)
    
    window_duration = pd.Timedelta(RESAMPLE_FREQUENCY)

    # --- Ká»ŠCH Báº¢N MÃ” PHá»ŽNG ---
    
    # Láº¥y thá»i Ä‘iá»ƒm log má»›i nháº¥t tá»« dá»¯ liá»‡u gá»‘c
    last_log_time = full_events_df.index.max()
    
    # Cá»­a sá»• thá»i gian mÃ´ phá»ng (vÃ­ dá»¥: 5 phÃºt sau log cuá»‘i cÃ¹ng)
    sim_start_time = last_log_time + window_duration
    sim_end_time = sim_start_time + window_duration 

    print("\n--- Báº¯t Ä‘áº§u MÃ´ Phá»ng PhÃ¡t Hiá»‡n Báº¥t ThÆ°á»ng Thá»i Gian Thá»±c ---")
    
    # VÃ­ dá»¥ 1: Ká»‹ch báº£n BÃŒNH THÆ¯á»œNG
    print(f"\n[1] GiÃ¡m sÃ¡t cá»­a sá»• BÃŒNH THÆ¯á»œNG ({sim_start_time} - {sim_end_time})")
    sim_normal_data = generate_new_log_batch(full_events_df, sim_start_time, sim_end_time, scenario="normal")
    anomalies_normal = detect_anomalies_realtime(anomaly_model, scaler, sim_normal_data)
    
    if anomalies_normal is None or anomalies_normal.empty:
        print("âœ… Káº¿t quáº£ (BÃŒNH THÆ¯á»œNG): KhÃ´ng phÃ¡t hiá»‡n báº¥t thÆ°á»ng.")
    else:
        print(f"âš ï¸ Káº¿t quáº£ (BÃŒNH THÆ¯á»œNG): PhÃ¡t hiá»‡n {len(anomalies_normal)} báº¥t thÆ°á»ng.")

    # VÃ­ dá»¥ 2: Ká»‹ch báº£n CÄ‚NG THáº²NG/Táº¤N CÃ”NG (STRESS)
    sim_start_time_stress = sim_end_time + window_duration # Cá»­a sá»• tiáº¿p theo
    sim_end_time_stress = sim_start_time_stress + window_duration

    print(f"\n[2] GiÃ¡m sÃ¡t cá»­a sá»• CÄ‚NG THáº²NG ({sim_start_time_stress} - {sim_end_time_stress})")
    sim_stress_data = generate_new_log_batch(full_events_df, sim_start_time_stress, sim_end_time_stress, scenario="stress")
    anomalies_stress = detect_anomalies_realtime(anomaly_model, scaler, sim_stress_data)

    if anomalies_stress is None or anomalies_stress.empty:
        print("âœ… Káº¿t quáº£ (STRESS): KhÃ´ng phÃ¡t hiá»‡n báº¥t thÆ°á»ng.")
    else:
        print(f"ðŸš¨ðŸš¨ Káº¾T QUáº¢ (STRESS): PhÃ¡t hiá»‡n {len(anomalies_stress)} báº¥t thÆ°á»ng! ðŸš¨ðŸš¨")
        print("Dá»¯ liá»‡u báº¥t thÆ°á»ng Ä‘Æ°á»£c chuáº©n hÃ³a (Top 3):")
        
        # Hiá»ƒn thá»‹ 3 hÃ ng Ä‘áº§u tiÃªn cá»§a cÃ¡c Ä‘áº·c trÆ°ng báº¥t thÆ°á»ng
        display_cols = [col for col in anomalies_stress.columns if col not in ['anomaly_score', 'anomaly']]
        
        # Sáº¯p xáº¿p vÃ  in ra
        top_anomalies = anomalies_stress.sort_values(by='anomaly_score', ascending=True).head(3)
        print(top_anomalies[['anomaly_score'] + display_cols].to_string())
